version: 0.0.1
jobId: "166"
jobName: Test Pyspark - State mngt iceberg-Miloni
jobType: Source Aligned Data Product
alias: Iceberg_read
discoveryPort:
  name: Test Pyspark - State mngt iceberg-Miloni
inputPorts:
  - alias: Weather_1
    description: Weather
    tags: []
    extra: {}
    syncType: pull
    type: s3-csv
    dataSetUrn: urn:dv:dataset:f4814824-5819-44ba-ac1d-4dc9a279a58a
    filter: ""
    projection: []
    persistDataFrame: false
    entity:
      advanceOptions:
        mergeSchema: false
productState:
  isDynamic: true
  alias: Iceberg_read
  retentionVersions: ""
  logicalSchema: ""
  stateStoreType: loadDataIceberg
  isProfilingEnabled: false
  updateStrategy: Overwrite
  tableName: s3source.weather_lg
  warehousePath: s3://bp-spark-sql-library-test-acc/
  catalogName: glue
  optional:
    persistDataFrame: false
    enableDataReconciliation: false
    enforceSchema: true
    enforceSchemaMethod: Warning
    catalogType: glue
  refreshInterval: 0 12 * * *
transformation:
  - alias: EMR_PySpark_1
    arguments:
      - s3://byte-etl-externaldemo/weather_data/##process_date##*.csv
    pythonFilePath: s3://bp-spark-sql-library-test-acc/custom-jobs/CustomPythonJobWriteParquet.py
    optional:
      pythonEnvTarGZPath: s3://byte-etl-externaldemo/pyspark_serverless_test/pyspark_venv.tar.gz
    type: customPySparkEMRServerless
    sequence: 2
    references:
      - alias: Weather_1
        sqlReference: ""
controlPort:
  dataQualityRules:
    NullValueCheck:
      productState:
        checks:
          - column: city
            expression: ==
            number: 0
        referenceAlias: Iceberg_read
outputPort:
  subscriptionChannels:
    - channelType: Postgres
      queryType: SQL
