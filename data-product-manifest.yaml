version: 0.0.1
jobId: "189"
jobName: redshift truncate insert miloni
jobType: DataSets
alias: s3
discoveryPort:
  name: redshift truncate insert miloni
inputPorts:
  - alias: Parquet_1
    isDynamic: true
    path: s3://byte-etl-externaldemo/weather_test_billing_parquet/
    optional:
      persistDataFrame: false
      advanceOptions:
        mergeSchema: true
      enableDataReconciliation: false
      enforceSchema: false
      connection: S3 with User
      dataSetUrn: ""
      dataProductUrn: ""
    type: inputParquet
productState:
  isDynamic: true
  alias: s3
  retentionVersions: ""
  logicalSchema: ""
  stateStoreType: writeDataFrameToRedshiftTable
  isProfilingEnabled: false
  updateStrategy: TruncateAndOverwrite
  connection: Redshift
  tableName: public.billing_1223
  endPoint: s3.us-east-1.amazonaws.com
  temporaryPathS3: s3a://bp-spark-sql-library-test-acc/testRedshiftUpload1/
  optional:
    persistDataFrame: false
    addRunIdColumn: false
    enableDataReconciliation: false
    enforceSchema: false
  refreshInterval: 0 12 * * *
transformation:
  - alias: Spark_SQL_1
    type: SQL
    description: s2
    query: select * from temp
    sequenceNo: 2
    references:
      - alias: Parquet_1
        sqlReference: temp
controlPort:
  dataQualityRules: {}
outputPort:
  subscriptionChannels:
    - channelType: Postgres
      queryType: SQL
